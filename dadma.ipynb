{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dadmatools.models.normalizer import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    عنوان مقاله: صفحهٔ اصلی\n",
      "\n",
      "    <p>متن ویکی‌پدیا فارسی</p>\n",
      "\n",
      "    من john هستم و در ب.م.م گیری تخصص دارم!\n",
      "\n",
      "    کلمات عربي مانند اصلاح كاف و يا ي برای توکنایزر ما اَهمیت دارند.\n",
      "\n",
      "    ما می  ‌دانیم که در تاریخ ۲۰ سپتامبر ۲۰۰۴ (۲۹ شهریور،۱۳۸۳) مقاله های \"ویکی پدیا\" در ۱۰۵ زبان به یک میلیون رسید.\n",
      "    که این مقالات شامل زمــــان های پیشین نیستند.\n",
      "\n",
      "    در ویکی‌پدیا فارسی ممکن است ( گاهی ) فاصله پرانتز ها رعایت نشده باشد، یا حتی ممکن است درباره محبوب ترین های فارسی صحبت شده باشد.\n",
      "    در اینجا یک ایمیل آزمایشی از من sh@sbu.ac.ir قرار دارد.\n",
      "\n",
      "    برای اطلاعات بیشتر می‌توانید به وبسایت ویکی‌پیدا فارسی به آدرس http://wikipedia.com سر بزنید.\n",
      "    (داخل پرانتز بگویم، این یک متن تستی است. حداقل به من اینطور گفته شده است.)\n",
      "\n",
      "    مجلهٔ تایم در گزارش سال ۲۰۰۶ خود، جیمی ویلز را در گروه ۱۰۰ فرد تأثیرگذار سال اعلام کرد.\n",
      "\n",
      "    همچنین در همین سال ویکی پدیای روسی برندهٔ جایزهٔ رانِت (روسی: Премия Рунета) در بخش «دانش و آموزش» شد. این جایزه از طرف دولت اعطا می شود.\n",
      "\n",
      "    همچنین ویکی پدیا جایزهٔ یک میلیون دلاریِ مدیریت پروژه را از همایش  صفاجو دریافت کرد.\n",
      "\n",
      "    پلتفورم اهداف توسعه پایدار United Nations:\n",
      "\n",
      "    چندین پروژهٔ متن-آزاد دارد که وظایف غیردانشنامه ای را انجام می دهند\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "    عنوان مقاله: صفحهٔ اصلی\n",
    "\n",
    "    <p>متن ویکی‌پدیا فارسی</p>\n",
    "\n",
    "    من john هستم و در ب.م.م گیری تخصص دارم!\n",
    "\n",
    "    کلمات عربي مانند اصلاح كاف و يا ي برای توکنایزر ما اَهمیت دارند.\n",
    "\n",
    "    ما می  ‌دانیم که در تاریخ ۲۰ سپتامبر ۲۰۰۴ (۲۹ شهریور،۱۳۸۳) مقاله های \"ویکی پدیا\" در ۱۰۵ زبان به یک میلیون رسید.\n",
    "    که این مقالات شامل زمــــان های پیشین نیستند.\n",
    "\n",
    "    در ویکی‌پدیا فارسی ممکن است ( گاهی ) فاصله پرانتز ها رعایت نشده باشد، یا حتی ممکن است درباره محبوب ترین های فارسی صحبت شده باشد.\n",
    "    در اینجا یک ایمیل آزمایشی از من sh@sbu.ac.ir قرار دارد.\n",
    "\n",
    "    برای اطلاعات بیشتر می‌توانید به وبسایت ویکی‌پیدا فارسی به آدرس http://wikipedia.com سر بزنید.\n",
    "    (داخل پرانتز بگویم، این یک متن تستی است. حداقل به من اینطور گفته شده است.)\n",
    "\n",
    "    مجلهٔ تایم در گزارش سال ۲۰۰۶ خود، جیمی ویلز را در گروه ۱۰۰ فرد تأثیرگذار سال اعلام کرد.\n",
    "\n",
    "    همچنین در همین سال ویکی پدیای روسی برندهٔ جایزهٔ رانِت (روسی: Премия Рунета) در بخش «دانش و آموزش» شد. این جایزه از طرف دولت اعطا می شود.\n",
    "\n",
    "    همچنین ویکی پدیا جایزهٔ یک میلیون دلاریِ مدیریت پروژه را از همایش  صفاجو دریافت کرد.\n",
    "\n",
    "    پلتفورم اهداف توسعه پایدار United Nations:\n",
    "\n",
    "    چندین پروژهٔ متن-آزاد دارد که وظایف غیردانشنامه ای را انجام می دهند\n",
    "    \"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عنوان مقاله صفحهٔ اصلی متن ویکی‌پدیا فارسی من john هستم و در ب م م گیری تخصص دارم کلمات عربی مانند اصلاح کاف و یا ی برای توکنایزر ما اَهمیت دارند ما می ‌دانیم که در تاریخ 20 سپتامبر 2004 29 شهریور 1383 مقاله های ویکی پدیا در 105 زبان به یک میلیون رسید که این مقالات شامل زمــــان های پیشین نیستند در ویکی‌پدیا فارسی ممکن است گاهی فاصله پرانتز ها رعایت نشده باشد یا حتی ممکن است درباره محبوب ترین های فارسی صحبت شده باشد در اینجا یک ایمیل آزمایشی از من <EMAIL> قرار دارد برای اطلاعات بیشتر می‌توانید به وبسایت ویکی‌پیدا فارسی به آدرس سر بزنید داخل پرانتز بگویم این یک متن تستی است حداقل به من اینطور گفته شده است مجلهٔ تایم در گزارش سال 2006 خود جیمی ویلز را در گروه 100 فرد تاثیرگذار سال اعلام کرد همچنین در همین سال ویکی پدیای روسی برندهٔ جایزهٔ رانِت روسی Премия Рунета در بخش «دانش و آموزش» شد این جایزه از طرف دولت اعطا می شود همچنین ویکی پدیا جایزهٔ یک میلیون دلاریِ مدیریت پروژه را از همایش صفاجو دریافت کرد پلتفورم اهداف توسعه پایدار United Nations چندین پروژهٔ متن آزاد دارد که وظایف غیردانشنامه ای را انجام می دهند\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer(\n",
    "    remove_puncs=True,\n",
    "    remove_html=True,\n",
    "    replace_email_with=\"<EMAIL>\",\n",
    "    replace_number_with=None,\n",
    "    replace_url_with=\"\",\n",
    "    replace_mobile_number_with=None,\n",
    "    replace_emoji_with=None,\n",
    "    replace_home_number_with=None)\n",
    "\n",
    "normalized_text = normalizer.normalize(text)\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nima Taheri\\Desktop\\katan\\6\\dadman.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nima%20Taheri/Desktop/katan/6/dadman.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlanguage\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nima%20Taheri/Desktop/katan/6/dadman.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# here lemmatizer and pos tagger will be loaded\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nima%20Taheri/Desktop/katan/6/dadman.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# as tokenizer is the default tool, it will be loaded as well even without calling\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nima%20Taheri/Desktop/katan/6/dadman.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pips \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtok,lem\u001b[39m\u001b[39m'\u001b[39m \n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\pipeline\\language.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmw_tokenizer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmwt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlemmatizer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlemmatizer\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpostagger\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtagger\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdependancy_parser\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdp\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstituency_parser\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mconspars\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\models\\postagger.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m \u001b[39mimport\u001b[39;00m embeddings \u001b[39mas\u001b[39;00m Embeddings\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcopy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dictionary\n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\models\\flair\\__init__.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m visual\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m trainers\n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\models\\flair\\models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msequence_tagger_model\u001b[39;00m \u001b[39mimport\u001b[39;00m SequenceTagger, FastSequenceTagger\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdependency_model\u001b[39;00m \u001b[39mimport\u001b[39;00m SemanticDependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlanguage_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageModel\n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\models\\flair\\models\\sequence_tagger_model.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dictionary, Sentence, Token, Label\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m TokenEmbeddings\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfile_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m cached_path\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Tuple, Union\n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\models\\flair\\embeddings.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mflair\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Corpus\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m LockedDropout, WordDropout\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dictionary, Token, Sentence\n\u001b[0;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfile_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m cached_path, open_inside_zip\n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\models\\flair\\nn.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataPoint\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m Result\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m flair\n",
      "File \u001b[1;32mc:\\Users\\Nima Taheri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dadmatools\\models\\flair\\training_utils.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdadmatools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mflair\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dictionary, Sentence\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m reduce\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m pearsonr, spearmanr\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m abstractmethod\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import dadmatools.pipeline.language as language\n",
    "\n",
    "# here lemmatizer and pos tagger will be loaded\n",
    "# as tokenizer is the default tool, it will be loaded as well even without calling\n",
    "pips = 'tok,lem' \n",
    "nlp = language.Pipeline(pips)\n",
    "\n",
    "# you can see the pipeline with this code\n",
    "print(nlp.analyze_pipes(pretty=True))\n",
    "\n",
    "# doc is an SpaCy object\n",
    "doc = nlp('از قصهٔ کودکیشان که می‌گفت، گاهی حرص می‌خورد!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
